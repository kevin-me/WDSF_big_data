我对大数据的认知和思考

大数据无疑是现在最火的词，可能大家并没有深入的了解它。现在人们正在处于一个信息爆炸的时代，我们每个人时时刻刻的都在产生数据，那我们产生的数据到底是什么呢？
其实主要来自于我们的智能电子设备 手机、 电脑、 平板 等 例如 手机刷淘宝的每一次点击 每一次浏览 刷抖音 看微博 读知乎 以及 每天疫情扫码 等等 每一个人 无时无刻不在
产生数据，全球60亿人口 数据量每天都在指数级别的增长。大家肯定好奇 产生的这些数据 被存储在哪里呢 这些数据要怎么应用呢 以及这些数据对我们有哪些好处呢

我们带着这些疑问一个问题一个问题的为大家解答。

1.产生的这些数据 存储在哪里呢

 要想解答这个问题，首先我们先揭秘一下数据是如何抓取的，拿手机中我们常用的app来说，数据主要分为 用户操作app产生的数据 称为用户行为数据 app本身的业务流程产生的数据 业务数据等

 用户操作app产生的数据 设计到一个概念 就是 “埋点”，我们操作app的动作都被 埋点 记录下来的，那么埋点 又是什么呢 怎么记录的 用什么记录的

 其实 埋点 就是 一种 你看不见的 数据收集方式 ，我给大家举个例子 就清晰了 拿淘宝登陆动作来讲 当你输入用户名 密码 在触发点击登陆的按钮时 你的登陆操作就会被记录一条日志

 用户A 在某时间某刻 登陆了淘宝 再比如 你把喜欢的衣服 加入购物车 这个操作也会被 预先 设计好的埋点方案 抓取 记录下来 。

 现在我们明白我们的操作数据是被预先设计好的埋点方案抓取走了 那么 他被存储到哪里去了 存储的方式和存储的介质也多种多样 我们列举几个行业能常见的

 1.用户操作数据记录到系统运行的日志文件 也就是被记录在服务器的文件中

 2.用户操作数据记录到数据库中 mysql redis Hbase Hdfs Elasticsearch  等 Elasticsearch 不建议哟 最近被黑客攻击被盗取大量的用户数据 中国还有50万人被盗取 （热点新闻可以查查）

 3.用户操作数据入kafKa kafka 是个消息队列 用于实时数据处理  （大数据处理的常用框架 我最近在学习kafka的源码  kafka 分为 生产者（推荐：源码最值得看 多线程 NIO 设计模式 要掌握 都会设计 很多面试题 也都是在 源码中 里面有很多宝藏 希望大家一起坚持去挖掘） 消费者  服务端  ）


 2.我们知道了 数据存储在哪里了 这些数据我们怎么使用呢

  这时候就要靠你的想象力了 其实大数据在技术层面就是个处理大量数据的工具 让我们具备了处理大量数据的能力 关键 还是在于我们想要解决什么问题 现在这是社会不在以解决问题为主导的了

  是以提出问题 谁能提出好的问题变得越来越关键    对未来的社会  是想象力的竞争和创造力的竞争  然而 痛点 问题 不满 苦难 都孕育着机会 。

  其实大数据不是用来解决问题的 而是找出数据之间的规律去发现问题 从而帮助我们进行决策 和 预测

  列举目前的一些公司常做的和我经历过的项目：

  广告公司 利用人们 收看习惯 兴趣类别 物理属性（男 女 年龄 区域 等等 ）等 给人贴标签  通俗的讲 就是 爱看体育类节目的人 推送 体育类型的广告 这就是人们常说的人物画像

  餐饮企业 分析企业 经营数据 每天门店的顾客数量 消费水平 以及 就餐体验 等等 发现经营中存在的问题 比如 菜品库存不够  菜品价格的调整 促销活动的反馈 等等

  打车软件 解决人找不到车  车找不到人的问题   推荐合理的上车地点

  疫情的检测  分析大量的病例CT影像 对疑似病例的病毒样本进行全基因组序列分析比对，能够有效防止病毒变异产生的漏检，并将原需数小时的全基因分析流程减少到半小时，大幅提高疑似病例的确诊速度和准确率

  以及我们每天出行必备的扫二维码 其实就是把我们去过的地方 记录 开始时间 开始地点  结束时间 结束地点  一旦 在你的出行轨迹中出现 疫情人员 你的健康码就会变颜色 都是通过大数据处理

  所以还是提醒大家 出行多扫码 戴口罩 对自己负责 对大家负责

  以上只是我们分享的几个案例 没有真正讲如何去实现的呢？

  接下来 我们就简单叙述下 （注 涉及到专业内容 简单了解 不懂可以私信我） 分如下几个步骤 ：

  1.收集数据来源 为了集成数据

  数据提取
  挑战:获取源数据是集成过程中的第一步。但是，如果数据源具有不同的格式、结构和类型，则会非常复杂和耗时。而且，一旦提取了数据，就必须在集成之前对其进行转换，使其与目标系统兼容。
  解决方案:最好的方法是创建一个您的组织将定期处理的资源列表。寻找支持从所有这些源提取的集成工具。最好使用支持结构化、非结构化和半结构化源的工具，以简化和简化提取过程。

  数据完整性
  挑战:数据质量是每个数据集成策略的主要关注点。糟糕的数据质量可能是影响整个集成周期的复合问题。处理无效或不正确的数据可能导致错误的分析，如果向下传递，可能会破坏结果。
  解决方案:为了确保正确和准确的数据进入数据管道，在开始项目之前创建一个数据质量管理计划。列出这些步骤可以确保在从开发到处理的数据管道的每个步骤中都保留了错误的数据。


  可伸缩性
  挑战:数据异构导致来自不同来源的数据流入统一的系统，最终导致数据量呈指数增长。为了应对这一挑战，组织需要采用健壮的集成解决方案，该方案具有处理大量数据和数据差异的特性，同时又不会影响性能。
  解决方案:预测企业数据增长的程度可以帮助组织选择满足可伸缩性和多样性需求的正确的集成解决方案。在这种情况下，采用分段方法也是有益的，因为一次集成一个数据点。根据总体集成策略评估每个数据点的价值可以帮助确定优先级和计划。
  例如，企业希望合并来自三个不同来源的数据:Salesforce、SQL Server和Excel文件。每个系统中的数据可以归类为独特的数据集，如销售、客户信息和财务数据。确定优先级并一次一个地集成这些数据集可以帮助组织逐步扩展数据处理。

  2.




Koalas，目标是使用 Pandas API 可以直接运行在 Spark，能够支持数据科学家更好的无缝迁移到 Spark。





5G 时代的到来推动了什么呢？

 提取数据的速度 越来越快  并发能力 大大提升

大数据跟人工智能有什么关联呢？
  病毒的预测

  药物的诊断

  机器人


大数据的技术那么多，需要全会吗

答案肯定是否定的，框架那么多，版本更新快，自学难度大  但是一些 技术硬实力的东西必须掌握

我们怎么保证自己的优势呢 找到自己研究的方向  也就是研究一个点

在一个点上 研究明白 研究深入  能 解决 问题  成为这个领域的专家 就是自己优势

那怎么寻找研究的方向呢

工具和方向  相辅相成


很多知识 不是学了没有用 是知识你还没机会用到它 跟大家分享下 那些知识是哪里用到的 给大家提个醒 （因为我也很希望有人告诉你 你学的有用 在哪里用到 然而我却是 自己发现的）

第一 基础最重要

第二 jvm调优 flink 的 状态管理 是放在 堆内存的哟    不精通的话 怎么调优处理 内存溢出怎么办

第三 高并发 设计模式 NIO  kafka 生产者源码 都有涉及哟  基础打不牢固 看着很费劲


读万卷书 不如行万里路 知识还是要学以致用 既要多读书 也要出去找找机会 ，听过大神说过的一句话 所有困难的问题之所有变得简单 是因为 每一天 坚持，失败是一种财富，
简单的人只想我要做什么 怎么做，而反思能让我们做的越来越好

不管你现在的生命是怎么样的，我们一定要有水的精神  像水一样不断地积蓄自己的力量，不断地冲破障碍。
当你发现时机不到的时候，把自己的厚度给积累起来，当有一天时机来临的时候， 你就能够奔腾入海，成就自己的生命。

我们这一生很短，我们终将会失去它，所以不妨大胆一点， 爱一个人，攀一座山，追一次梦。